#!/usr/bin/env python3
"""
Script pour nettoyer la BDD et la peupler avec des donn√©es fra√Æches
√âtapes:
1. Nettoyer la table offers dans Supabase
2. Lancer les scrapers FreeWork et Collective.work (15 pages chacun)
3. Transformer et charger les donn√©es dans Supabase
"""

import os
import sys
import subprocess
import time
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client

# Configuration des chemins
PROJECT_ROOT = Path(__file__).parent.parent.parent
SCRAPER_DIR = PROJECT_ROOT / 'scraper'
ETL_DIR = PROJECT_ROOT / 'etl'
DATA_DIR = SCRAPER_DIR / 'data'

# Ajouter les modules au path
sys.path.insert(0, str(ETL_DIR))
sys.path.insert(0, str(SCRAPER_DIR))

# Charger les variables d'environnement
load_dotenv(ETL_DIR / '.env')

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")


def print_step(step_num, title, emoji="üìå"):
    """Affiche une √©tape format√©e"""
    print(f"\n{'='*70}")
    print(f"{emoji} √âTAPE {step_num}: {title}")
    print(f"{'='*70}\n")


def clean_database():
    """Nettoie compl√®tement la table offers"""
    print_step(1, "Nettoyage de la base de donn√©es", "üóëÔ∏è")
    
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
        
        # R√©cup√©rer toutes les offres pour les supprimer
        result = supabase.table('offers').select('id').execute()
        count = len(result.data)
        
        print(f"   üìä Offres actuelles: {count}")
        
        if count > 0:
            # Supprimer toutes les offres une par une ou par batch
            supabase.table('offers').delete().gte('id', 0).execute()
            print(f"   ‚úÖ {count} offres supprim√©es")
        else:
            print(f"   ‚ÑπÔ∏è  Base de donn√©es d√©j√† vide")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors du nettoyage: {e}")
        # On continue quand m√™me
        return True


def clean_scraper_data():
    """Nettoie les anciens fichiers de scraping"""
    print_step(2, "Nettoyage des anciens fichiers de scraping", "üßπ")
    
    try:
        if DATA_DIR.exists():
            for json_file in DATA_DIR.glob('*.json'):
                json_file.unlink()
                print(f"   Supprim√©: {json_file.name}")
        
        print(f"‚úÖ Fichiers de scraping nettoy√©s")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors du nettoyage des fichiers: {e}")
        return False


def run_scrapers():
    """Lance les scrapers FreeWork et Collective.work"""
    print_step(3, "Lancement des scrapers (15 pages par source)", "üï∑Ô∏è")
    
    scrapers = [
        ('freework', 'FreeWork'),
        ('collective_work', 'Collective.work')
    ]
    
    for spider_name, display_name in scrapers:
        print(f"\nüöÄ Scraping {display_name}...")
        print(f"   Spider: {spider_name}")
        print(f"   Pages: 15")
        print(f"   D√©marrage: {datetime.now().strftime('%H:%M:%S')}")
        
        try:
            # Lancer le scraper avec PowerShell depuis le bon r√©pertoire
            output_file = DATA_DIR / f'{spider_name}.json'
            
            cmd = f'cd "{SCRAPER_DIR}" ; scrapy crawl {spider_name} -O "{output_file}" --loglevel=INFO'
            
            result = subprocess.run(
                ['powershell', '-Command', cmd],
                capture_output=True,
                text=True,
                timeout=600  # 10 minutes max par scraper
            )
            
            if result.returncode == 0 or output_file.exists():
                # Compter les offres scrap√©es
                if output_file.exists():
                    import json
                    try:
                        with open(output_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            count = len(data) if isinstance(data, list) else 1
                        
                        print(f"   ‚úÖ {display_name}: {count} offres scrap√©es")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Fichier cr√©√© mais erreur de lecture: {e}")
                else:
                    print(f"   ‚ö†Ô∏è  Fichier de sortie non trouv√©")
            else:
                print(f"   ‚ùå Erreur lors du scraping de {display_name}")
                if result.stderr:
                    print(f"   {result.stderr[:300]}")
                
        except subprocess.TimeoutExpired:
            print(f"   ‚è∞ Timeout pour {display_name} (10 minutes d√©pass√©es)")
        except Exception as e:
            print(f"   ‚ùå Erreur: {e}")
    
    print(f"\n‚úÖ Scraping termin√©: {datetime.now().strftime('%H:%M:%S')}")
    return True


def load_to_database():
    """Charge les donn√©es scrap√©es dans Supabase"""
    print_step(4, "Chargement dans Supabase", "üì§")
    
    os.chdir(ETL_DIR)
    
    try:
        # Utiliser le script ETL existant
        from run_full_pipeline import main as run_etl
        
        print("üîÑ Transformation et validation des donn√©es...")
        result = run_etl()
        
        if result:
            print(f"‚úÖ Donn√©es charg√©es avec succ√®s dans Supabase")
            return True
        else:
            print(f"‚ö†Ô∏è  Chargement termin√© avec des avertissements")
            return True
            
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement: {e}")
        import traceback
        traceback.print_exc()
        return False


def verify_data():
    """V√©rifie les donn√©es charg√©es"""
    print_step(5, "V√©rification des donn√©es", "‚úÖ")
    
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
        
        # Compter les offres par source
        result = supabase.table('offers').select('source', count='exact').execute()
        total = len(result.data)
        
        print(f"üìä Total des offres dans la BDD: {total}")
        
        # D√©tail par source
        sources = {}
        for offer in result.data:
            source = offer.get('source', 'unknown')
            sources[source] = sources.get(source, 0) + 1
        
        print(f"\nüìã R√©partition par source:")
        for source, count in sources.items():
            print(f"   ‚Ä¢ {source}: {count} offres")
        
        # V√©rifier les villes
        locations_result = supabase.table('offers').select('location').execute()
        locations = set(offer['location'] for offer in locations_result.data if offer.get('location'))
        
        print(f"\nüèôÔ∏è  Villes uniques: {len(locations)}")
        print(f"   Exemples: {', '.join(list(locations)[:10])}")
        
        # V√©rifier les technologies
        tech_result = supabase.table('offers').select('technologies').execute()
        all_techs = set()
        for offer in tech_result.data:
            if offer.get('technologies'):
                all_techs.update(offer['technologies'])
        
        print(f"\nüíª Technologies uniques: {len(all_techs)}")
        print(f"   Exemples: {', '.join(list(all_techs)[:15])}")
        
        # TJM moyen
        tjm_result = supabase.table('offers').select('tjm_min, tjm_max').execute()
        valid_tjms = [
            (offer['tjm_min'] + offer['tjm_max']) / 2
            for offer in tjm_result.data
            if offer.get('tjm_min') and offer.get('tjm_max')
        ]
        
        if valid_tjms:
            avg_tjm = sum(valid_tjms) / len(valid_tjms)
            print(f"\nüí∞ TJM moyen: {avg_tjm:.0f}‚Ç¨/jour")
            print(f"   TJM min: {min(valid_tjms):.0f}‚Ç¨")
            print(f"   TJM max: {max(valid_tjms):.0f}‚Ç¨")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification: {e}")
        return False


def main():
    """Pipeline complet"""
    print("\n" + "="*70)
    print("üöÄ AWA - Pipeline de peuplement de la base de donn√©es")
    print("="*70)
    print(f"üìÖ D√©marrage: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = time.time()
    
    # V√©rifier la configuration
    if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
        print("‚ùå Variables d'environnement Supabase manquantes")
        print("   V√©rifiez SUPABASE_URL et SUPABASE_SERVICE_ROLE_KEY dans .env")
        return False
    
    steps = [
        (clean_database, "Nettoyage BDD"),
        (clean_scraper_data, "Nettoyage fichiers"),
        (run_scrapers, "Scraping"),
        (load_to_database, "Chargement BDD"),
        (verify_data, "V√©rification")
    ]
    
    for step_func, step_name in steps:
        try:
            success = step_func()
            if not success:
                print(f"\n‚ö†Ô∏è  {step_name} a √©chou√©, mais on continue...")
        except Exception as e:
            print(f"\n‚ùå Erreur critique dans {step_name}: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    duration = time.time() - start_time
    
    print("\n" + "="*70)
    print(f"‚úÖ Pipeline termin√© avec succ√®s!")
    print(f"‚è±Ô∏è  Dur√©e totale: {duration:.1f} secondes ({duration/60:.1f} minutes)")
    print(f"üìÖ Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70 + "\n")
    
    return True


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
