# ğŸ”„ Cycle de Vie du Logiciel AWA - Analyse ComplÃ¨te

## ğŸ“‹ Ã‰tat Actuel du Projet

### âœ… Composants Fonctionnels
- **Scraper** : Scrapy fonctionnel (16 offres Freework collectÃ©es)
- **ETL** : Pipeline opÃ©rationnel (41 records traitÃ©s, 23 offres en base)
- **Base de donnÃ©es** : Supabase configurÃ© et connectÃ©
- **Frontend** : Structure Next.js avec connexion Supabase
- **Configuration** : SystÃ¨me centralisÃ© refactorisÃ©

### âš ï¸ Composants Manquants/Incomplets
- **Service Scheduler** : Absent (rÃ©fÃ©rencÃ© dans docker-compose mais pas implÃ©mentÃ©)
- **Cron Jobs** : Pas d'orchestration automatique
- **Frontend complet** : API routes existantes mais composants UI Ã  finaliser
- **DÃ©ploiement** : GitHub Actions configurÃ© mais dÃ©ploiement non implÃ©mentÃ©

## ğŸ”„ ScÃ©nario Type du Cycle de Vie

### 1. **DÃ©veloppement Local** 
```bash
# Setup initial
python scripts/setup.py --interactive
python scripts/env_manager.py validate

# Tests manuels
cd services/scraper && python -m scrapy crawl freework
cd services/etl && python run_simple_etl.py
cd services/frontend && npm run dev
```

### 2. **IntÃ©gration Continue (GitHub Actions)**
```yaml
# DÃ©clenchÃ© sur : push main, pull requests
on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

# Pipeline :
1. test-scraper â†’ Tests unitaires Python/Scrapy
2. test-frontend â†’ Tests Jest/TypeScript, linting
3. build-and-push â†’ Build Docker images et push vers GHCR
4. deploy â†’ DÃ©ploiement production (non implÃ©mentÃ©)
```

### 3. **Production (ScÃ©nario Cible)**
```bash
# DÃ©ploiement via Docker Compose
docker-compose up -d

# Services actifs :
- scraper:8000     (Scrapy + API)  
- etl:8001         (Pipeline ETL)
- scheduler:8002   (Cron orchestration) âš ï¸ MANQUANT
- frontend:3000    (Next.js dashboard)
- monitoring:9100  (Node exporter)
```

## ğŸ• Orchestration par Cron (PROBLÃˆME IDENTIFIÃ‰)

### âŒ ProblÃ¨me Actuel
Le **service scheduler** est rÃ©fÃ©rencÃ© dans le `docker-compose.yml` mais **n'existe pas** :

```yaml
# services/scheduler/Dockerfile - INEXISTANT
# services/scheduler/crontab - INEXISTANT  
# services/scheduler/scripts/ - INEXISTANT
```

### ğŸ¯ Cron Workflow Cible (Ã€ ImplÃ©menter)
```bash
# Crontab dans le service scheduler
# Extraction hebdomadaire le dimanche Ã  2h
0 2 * * 0 /app/scripts/run_scraper.sh freework
0 3 * * 0 /app/scripts/run_scraper.sh malt  
0 4 * * 0 /app/scripts/run_scraper.sh comet

# ETL quotidien Ã  6h
0 6 * * * /app/scripts/run_etl.sh

# Nettoyage mensuel le 1er Ã  1h
0 1 1 * * /app/scripts/cleanup_old_data.sh
```

## ğŸ”— Connexion Frontend â†” Supabase

### âœ… Configuration Existante
```typescript
// lib/supabase.ts - FONCTIONNEL
export const supabase = createClient(supabaseUrl, supabaseKey)
export const createServerClient = () => createClient(url, serviceRoleKey)
```

### âœ… API Routes Existantes
```typescript
// app/api/offers/route.ts - FONCTIONNEL
GET /api/offers?limit=50&tech=React&location=Paris
POST /api/offers (crÃ©ation d'offres)

// app/api/stats/route.ts - Ã€ VÃ‰RIFIER
GET /api/stats (statistiques globales)
```

### âš ï¸ Composants UI Manquants
```typescript
// Ã€ crÃ©er/complÃ©ter :
- Dashboard principal avec graphiques TJM
- Filtres par technologie/localisation  
- Tables d'offres avec pagination
- Graphiques Ã©volution temporelle
- Cartes/gÃ©olocalisation
```

## ğŸ“Š Flux de DonnÃ©es Complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sites TJM     â”‚â”€â”€â”€â–¶â”‚   Scrapers   â”‚â”€â”€â”€â–¶â”‚     ETL     â”‚â”€â”€â”€â–¶â”‚   Supabase   â”‚â”€â”€â”€â–¶â”‚  Frontend   â”‚
â”‚                 â”‚    â”‚              â”‚    â”‚             â”‚    â”‚              â”‚    â”‚             â”‚
â”‚ â€¢ Freework      â”‚    â”‚ â€¢ Scrapy     â”‚    â”‚ â€¢ Transform â”‚    â”‚ â€¢ PostgreSQL â”‚    â”‚ â€¢ Next.js   â”‚
â”‚ â€¢ Malt          â”‚    â”‚ â€¢ Playwright â”‚    â”‚ â€¢ Validate  â”‚    â”‚ â€¢ RLS        â”‚    â”‚ â€¢ Dashboard â”‚
â”‚ â€¢ Comet         â”‚    â”‚ â€¢ JSON out   â”‚    â”‚ â€¢ Load      â”‚    â”‚ â€¢ 23 offers  â”‚    â”‚ â€¢ API       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–²                    â–²                                        
                              â”‚                    â”‚                                        
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 
                       â”‚  Scheduler   â”‚    â”‚  Monitoring â”‚                                 
                       â”‚              â”‚    â”‚             â”‚                                 
                       â”‚ â€¢ Cron jobs  â”‚    â”‚ â€¢ Logs      â”‚                                 
                       â”‚ â€¢ Webhook    â”‚    â”‚ â€¢ Metrics   â”‚                                 
                       â”‚ âš ï¸ MANQUANT  â”‚    â”‚ â€¢ Alerts    â”‚                                 
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 
```

## ğŸš§ Ã‰tapes Restantes pour Finaliser

### 1. **CrÃ©er le Service Scheduler** (CRITIQUE)

<function_calls>
<invoke name="create_file">
<parameter name="content">#!/bin/bash
# Script pour crÃ©er le service scheduler manquant

echo "ğŸ—ï¸ CrÃ©ation du service scheduler..."

# CrÃ©er la structure
mkdir -p services/scheduler/{scripts,cron,logs}

# Dockerfile pour le scheduler
cat > services/scheduler/Dockerfile << 'EOF'
FROM python:3.11-slim

# Installer cron
RUN apt-get update && apt-get install -y cron curl && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copier les requirements
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copier les scripts
COPY scripts/ ./scripts/
COPY cron/crontab /etc/cron.d/awa-scheduler

# Permissions
RUN chmod 0644 /etc/cron.d/awa-scheduler
RUN chmod +x ./scripts/*.sh

# Logs
RUN touch /var/log/cron.log

# DÃ©marrage
CMD cron && tail -f /var/log/cron.log
EOF

# Requirements du scheduler
cat > services/scheduler/requirements.txt << 'EOF'
requests==2.31.0
docker==6.1.3
python-crontab==3.0.0
pydantic==2.4.2
EOF

# Crontab
cat > services/scheduler/cron/crontab << 'EOF'
# AWA Scheduler Crontab
# Extraction hebdomadaire le dimanche Ã  2h
0 2 * * 0 /app/scripts/run_scraper.sh freework >> /var/log/cron.log 2>&1
0 3 * * 0 /app/scripts/run_scraper.sh malt >> /var/log/cron.log 2>&1  
0 4 * * 0 /app/scripts/run_scraper.sh comet >> /var/log/cron.log 2>&1

# ETL quotidien Ã  6h
0 6 * * * /app/scripts/run_etl.sh >> /var/log/cron.log 2>&1

# Nettoyage mensuel le 1er Ã  1h
0 1 1 * * /app/scripts/cleanup_old_data.sh >> /var/log/cron.log 2>&1

# Health check toutes les heures
0 * * * * /app/scripts/health_check.sh >> /var/log/cron.log 2>&1
EOF

# Script de lancement du scraper
cat > services/scheduler/scripts/run_scraper.sh << 'EOF'
#!/bin/bash
SCRAPER_URL="${SCRAPER_SERVICE_URL:-http://scraper:8000}"
SPIDER_NAME="${1:-freework}"

echo "$(date): Lancement scraper $SPIDER_NAME"

# DÃ©clencher le scraper via API
curl -X POST "$SCRAPER_URL/api/scrape" \
  -H "Content-Type: application/json" \
  -d "{\"spider\": \"$SPIDER_NAME\"}" \
  || echo "$(date): Erreur scraper $SPIDER_NAME"
EOF

# Script ETL
cat > services/scheduler/scripts/run_etl.sh << 'EOF'  
#!/bin/bash
ETL_URL="${ETL_SERVICE_URL:-http://etl:8001}"

echo "$(date): Lancement ETL"

# DÃ©clencher l'ETL via API
curl -X POST "$ETL_URL/api/process" \
  -H "Content-Type: application/json" \
  || echo "$(date): Erreur ETL"
EOF

# Script de nettoyage
cat > services/scheduler/scripts/cleanup_old_data.sh << 'EOF'
#!/bin/bash
echo "$(date): Nettoyage des anciennes donnÃ©es"

# Supprimer les fichiers de plus de 30 jours
find /app/data/raw -name "*.json" -mtime +30 -delete
find /app/logs -name "*.log" -mtime +7 -delete

echo "$(date): Nettoyage terminÃ©"
EOF

# Script health check
cat > services/scheduler/scripts/health_check.sh << 'EOF'
#!/bin/bash
SCRAPER_URL="${SCRAPER_SERVICE_URL:-http://scraper:8000}"
ETL_URL="${ETL_SERVICE_URL:-http://etl:8001}"

echo "$(date): Health check des services"

# Check scraper
if curl -f "$SCRAPER_URL/health" > /dev/null 2>&1; then
  echo "$(date): âœ… Scraper OK"
else
  echo "$(date): âŒ Scraper DOWN"
fi

# Check ETL  
if curl -f "$ETL_URL/health" > /dev/null 2>&1; then
  echo "$(date): âœ… ETL OK"
else
  echo "$(date): âŒ ETL DOWN"
fi
EOF

# Permissions
chmod +x services/scheduler/scripts/*.sh

echo "âœ… Service scheduler crÃ©Ã©!"
echo "ğŸ“‹ Prochaines Ã©tapes:"
echo "  1. ImplÃ©menter les endpoints /api/scrape et /api/process"
echo "  2. Tester : docker-compose up scheduler"
echo "  3. VÃ©rifier les logs : docker logs awa-scheduler"
